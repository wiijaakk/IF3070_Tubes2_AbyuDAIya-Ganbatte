{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection: Hybrid Approach\n",
    "**Tugas Besar 2 IF3070 â€“ Dasar Inteligensi Artifisial**\n",
    "\n",
    "**Team:** AbyuDAIya-Ganbatte\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a hybrid implementation:\n",
    "1.  **Data Preprocessing:** Uses industry-standard **Scikit-Learn** libraries for robust, efficient, and clean data transformation.\n",
    "2.  **Model:** Uses a **custom-built Logistic Regression** (implemented from scratch) featuring advanced optimization techniques like **Adam Optimizer** and **Focal Loss**.\n",
    "\n",
    "### Libraries Used\n",
    "* `pandas` & `numpy`: Data manipulation.\n",
    "* `sklearn.preprocessing`: StandardScaler, OneHotEncoder, RobustScaler.\n",
    "* `sklearn.impute`: KNNImputer, SimpleImputer.\n",
    "* `sklearn.compose`: ColumnTransformer.\n",
    "* `sklearn.model_selection`: train_test_split, StratifiedKFold.\n",
    "* `sklearn.metrics`: roc_auc_score, f1_score (for verification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-Learn Imports for Preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler, FunctionTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Manual Logistic Regression Model\n",
    "This section contains the custom implementation of Logistic Regression. **No libraries** are used for the optimization logic here. It includes:\n",
    "* **Adam Optimizer** (Adaptive Moment Estimation)\n",
    "* **Focal Loss** (for class imbalance)\n",
    "* **Elastic Net Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression implemented from scratch with Adam Optimizer and Focal Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, optimizer=\"adam\", \n",
    "                 batch_size=None, regularization=0.0, l1_ratio=0.0, class_weight=None,\n",
    "                 lr_schedule=\"constant\", lr_decay=0.1, lr_decay_steps=100,\n",
    "                 beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
    "                 use_focal_loss=False, focal_gamma=2.0,\n",
    "                 early_stopping=True, patience=10, tol=1e-5, verbose=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.regularization = regularization\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.class_weight = class_weight\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_decay_steps = lr_decay_steps\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.early_stopping = early_stopping\n",
    "        self.patience = patience\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        if batch_size is None:\n",
    "            self.batch_size = 32 if self.optimizer == \"mini-batch\" else None\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        \n",
    "        # Model parameters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "        # Adam parameters\n",
    "        self.m_w = None; self.v_w = None\n",
    "        self.m_b = None; self.v_b = None\n",
    "        self.t = 0\n",
    "        \n",
    "        self.loss_history = []\n",
    "    \n",
    "    def _get_learning_rate(self, iteration):\n",
    "        if self.lr_schedule == \"constant\": return self.initial_lr\n",
    "        elif self.lr_schedule == \"step\":\n",
    "            return self.initial_lr * (self.lr_decay ** (iteration // self.lr_decay_steps))\n",
    "        return self.initial_lr\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        positive_mask = z >= 0\n",
    "        negative_mask = ~positive_mask\n",
    "        result = np.zeros_like(z, dtype=float)\n",
    "        result[positive_mask] = 1 / (1 + np.exp(-z[positive_mask]))\n",
    "        exp_z = np.exp(z[negative_mask])\n",
    "        result[negative_mask] = exp_z / (1 + exp_z)\n",
    "        return result\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred, sample_weights=None):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        if self.use_focal_loss:\n",
    "            p_t = np.where(y_true == 1, y_pred, 1 - y_pred)\n",
    "            focal_weight = (1 - p_t) ** self.focal_gamma\n",
    "            loss_per_sample = -focal_weight * np.log(p_t)\n",
    "        else:\n",
    "            loss_per_sample = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        \n",
    "        if sample_weights is not None:\n",
    "            loss = np.sum(sample_weights * loss_per_sample) / np.sum(sample_weights)\n",
    "        else:\n",
    "            loss = np.mean(loss_per_sample)\n",
    "            \n",
    "        if self.regularization > 0 and self.weights is not None:\n",
    "            l1_term = self.l1_ratio * np.sum(np.abs(self.weights))\n",
    "            l2_term = (1 - self.l1_ratio) * 0.5 * np.sum(self.weights ** 2)\n",
    "            loss += self.regularization * (l1_term + l2_term)\n",
    "        return loss\n",
    "    \n",
    "    def _compute_gradients(self, X, y, y_pred, sample_weights=None):\n",
    "        n_samples = len(y)\n",
    "        if self.use_focal_loss:\n",
    "            epsilon = 1e-15\n",
    "            y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "            p_t = np.where(y == 1, y_pred_clipped, 1 - y_pred_clipped)\n",
    "            focal_weight = (1 - p_t) ** self.focal_gamma\n",
    "            grad_p = np.where(\n",
    "                y == 1,\n",
    "                -focal_weight * (self.focal_gamma * (1 - y_pred_clipped) * np.log(y_pred_clipped + epsilon) + 1) / (y_pred_clipped + epsilon),\n",
    "                focal_weight * (self.focal_gamma * y_pred_clipped * np.log(1 - y_pred_clipped + epsilon) + 1) / (1 - y_pred_clipped + epsilon)\n",
    "            )\n",
    "            error = grad_p * y_pred_clipped * (1 - y_pred_clipped)\n",
    "        else:\n",
    "            error = y_pred - y\n",
    "        \n",
    "        if sample_weights is not None:\n",
    "            weighted_error = sample_weights * error\n",
    "            dw = np.dot(X.T, weighted_error) / np.sum(sample_weights)\n",
    "            db = np.sum(weighted_error) / np.sum(sample_weights)\n",
    "        else:\n",
    "            dw = (1 / n_samples) * np.dot(X.T, error)\n",
    "            db = (1 / n_samples) * np.sum(error)\n",
    "            \n",
    "        if self.regularization > 0:\n",
    "            l2_grad = (1 - self.l1_ratio) * self.weights\n",
    "            l1_grad = self.l1_ratio * np.sign(self.weights)\n",
    "            dw += self.regularization * (l1_grad + l2_grad)\n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Handle inputs\n",
    "        if hasattr(X, \"toarray\"): X = X.toarray()  # Handle sparse matrices from sklearn\n",
    "        if isinstance(X, pd.DataFrame): X = X.values\n",
    "        if isinstance(y, (pd.Series, pd.DataFrame)): y = y.values.flatten()\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        np.random.seed(42)\n",
    "        self.weights = np.random.randn(n_features) * np.sqrt(2.0 / n_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        # Initialize Adam\n",
    "        self.m_w = np.zeros(n_features); self.v_w = np.zeros(n_features)\n",
    "        self.m_b = 0.0; self.v_b = 0.0\n",
    "        self.t = 0\n",
    "        \n",
    "        # Class weights\n",
    "        if self.class_weight == \"balanced\":\n",
    "            class_counts = np.bincount(y.astype(int))\n",
    "            cw = n_samples / (2 * class_counts)\n",
    "            sample_weights = np.where(y == 1, cw[1], cw[0])\n",
    "        else:\n",
    "            sample_weights = None\n",
    "            \n",
    "        # Training Loop (Adam Only)\n",
    "        best_loss = float('inf'); patience_counter = 0; best_weights = self.weights.copy()\n",
    "        batch_size = min(self.batch_size if self.batch_size else 32, n_samples)\n",
    "        \n",
    "        for iteration in range(self.n_iterations):\n",
    "            lr = self._get_learning_rate(iteration)\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_s, y_s = X[indices], y[indices]\n",
    "            sw_s = sample_weights[indices] if sample_weights is not None else None\n",
    "            \n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                X_batch = X_s[start_idx:end_idx]\n",
    "                y_batch = y_s[start_idx:end_idx]\n",
    "                sw_batch = sw_s[start_idx:end_idx] if sw_s is not None else None\n",
    "                \n",
    "                # Forward & Backward\n",
    "                z = np.dot(X_batch, self.weights) + self.bias\n",
    "                y_pred = self.sigmoid(z)\n",
    "                dw, db = self._compute_gradients(X_batch, y_batch, y_pred, sw_batch)\n",
    "                \n",
    "                # Adam Update\n",
    "                self.t += 1\n",
    "                self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * dw\n",
    "                self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * db\n",
    "                self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (dw ** 2)\n",
    "                self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (db ** 2)\n",
    "                \n",
    "                m_w_corr = self.m_w / (1 - self.beta1 ** self.t)\n",
    "                m_b_corr = self.m_b / (1 - self.beta1 ** self.t)\n",
    "                v_w_corr = self.v_w / (1 - self.beta2 ** self.t)\n",
    "                v_b_corr = self.v_b / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                self.weights -= lr * m_w_corr / (np.sqrt(v_w_corr) + self.epsilon)\n",
    "                self.bias -= lr * m_b_corr / (np.sqrt(v_b_corr) + self.epsilon)\n",
    "            \n",
    "            # Evaluation for Early Stopping\n",
    "            z_full = np.dot(X, self.weights) + self.bias\n",
    "            loss = self.compute_loss(y, self.sigmoid(z_full), sample_weights)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            if self.early_stopping:\n",
    "                if loss < best_loss - self.tol:\n",
    "                    best_loss = loss; best_weights = self.weights.copy(); patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        if self.verbose: print(f\"Early stopping at {iteration}, loss: {loss:.6f}\")\n",
    "                        self.weights = best_weights\n",
    "                        break\n",
    "                        \n",
    "            if self.verbose and iteration % 500 == 0:\n",
    "                print(f\"Iteration {iteration}: loss = {loss:.6f}\")\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(X, \"toarray\"): X = X.toarray()\n",
    "        if isinstance(X, pd.DataFrame): X = X.values\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Creation (Pandas)\n",
    "We still use Pandas for creating *new* features (Feature Engineering) because Scikit-Learn is designed for processing *existing* features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Creates domain-specific ratios and interactions.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ratios (Numerical/Numerical)\n",
    "    if 'transaction_amount' in df.columns and 'avg_transaction_amount' in df.columns:\n",
    "        df['amount_vs_avg'] = df['transaction_amount'] / (df['avg_transaction_amount'] + 1e-5)\n",
    "    \n",
    "    if 'transactions_last_1h' in df.columns and 'transactions_last_24h' in df.columns:\n",
    "        df['hourly_conc'] = df['transactions_last_1h'] / (df['transactions_last_24h'] + 1)\n",
    "        \n",
    "    # Log Transformations for skewed features\n",
    "    for col in ['transaction_amount', 'distance_from_home']:\n",
    "        if col in df.columns and df[col].min() >= 0:\n",
    "            df[f'{col}_log'] = np.log1p(df[col])\n",
    "            \n",
    "    # Interaction Features\n",
    "    if 'ip_risk_score' in df.columns and 'device_trust_score' in df.columns:\n",
    "        df['risk_interaction'] = df['ip_risk_score'] * (1 - df['device_trust_score'] / 100)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Processing Pipeline (Scikit-Learn)\n",
    "This replaces the manual `one_hot_encode`, `StandardScaler`, and `KNNImputer` functions with a robust `ColumnTransformer`.\n",
    "\n",
    "**Strategy:**\n",
    "* **Numerical Cols:** Impute (KNN) -> Scale (RobustScaler to handle outliers).\n",
    "* **Categorical Cols:** Impute (Mode) -> One-Hot Encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessor(numerical_cols, categorical_cols):\n",
    "    \"\"\"\n",
    "    Builds a Scikit-Learn ColumnTransformer.\n",
    "    \"\"\"\n",
    "    # Pipeline for Numerical Features\n",
    "    # 1. KNN Imputer fills missing values based on neighbors\n",
    "    # 2. RobustScaler scales data but is robust to outliers (uses IQR instead of mean/std)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        ('scaler', RobustScaler())  # Better than StandardScaler for fraud data (outliers)\n",
    "    ])\n",
    "\n",
    "    # Pipeline for Categorical Features\n",
    "    # 1. SimpleImputer fills missing with 'most_frequent' (Mode)\n",
    "    # 2. OneHotEncoder converts categories to binary columns\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Combine both\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ],\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution\n",
    "Orchestrates loading, splitting (sklearn), preprocessing (sklearn), and training (manual model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Features...\n",
      "Numerical: 26, Categorical: 6\n",
      "Running Sklearn Pipeline...\n",
      "Processed Train Shape: (80000, 59)\n",
      "Training Manual Logistic Regression...\n",
      "Iteration 0: loss = 3.859026\n",
      "Early stopping at 10, loss: 17.436073\n",
      "\n",
      "Validation ROC AUC: 0.4525\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92     17174\n",
      "           1       0.21      0.00      0.00      2826\n",
      "\n",
      "    accuracy                           0.86     20000\n",
      "   macro avg       0.54      0.50      0.46     20000\n",
      "weighted avg       0.77      0.86      0.79     20000\n",
      "\n",
      "Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    \n",
    "    # Store IDs for submission\n",
    "    test_ids = test_df['ID']\n",
    "    \n",
    "    # Separate Target\n",
    "    X = train_df.drop(columns=['is_fraud', 'ID', 'transaction_id', 'user_id'], errors='ignore')\n",
    "    y = train_df['is_fraud']\n",
    "    X_test_raw = test_df.drop(columns=['ID', 'transaction_id', 'user_id'], errors='ignore')\n",
    "\n",
    "    # 2. Feature Engineering (Creation Only)\n",
    "    print(\"Creating Features...\")\n",
    "    X = create_features(X)\n",
    "    X_test_raw = create_features(X_test_raw)\n",
    "\n",
    "    # Identify Columns\n",
    "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    print(f\"Numerical: {len(num_cols)}, Categorical: {len(cat_cols)}\")\n",
    "\n",
    "    # 3. Split Data (Using Sklearn)\n",
    "    # Stratify ensures the fraud ratio is maintained in train/val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # 4. Fit & Transform Preprocessing Pipeline\n",
    "    print(\"Running Sklearn Pipeline...\")\n",
    "    preprocessor = get_preprocessor(num_cols, cat_cols)\n",
    "    \n",
    "    # Fit on TRAIN, Transform on TRAIN, VAL, and TEST\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    X_test_processed = preprocessor.transform(X_test_raw)\n",
    "    \n",
    "    print(f\"Processed Train Shape: {X_train_processed.shape}\")\n",
    "\n",
    "    # 5. Train Manual Model\n",
    "    print(\"Training Manual Logistic Regression...\")\n",
    "    model = ManualLogisticRegression(\n",
    "        learning_rate=0.0015, \n",
    "        n_iterations=3000, \n",
    "        optimizer=\"adam\", \n",
    "        batch_size=256, \n",
    "        regularization=0.0006, \n",
    "        l1_ratio=0.5,\n",
    "        class_weight=\"balanced\",\n",
    "        use_focal_loss=True,  # Using Focal Loss for imbalance\n",
    "        focal_gamma=2.0\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train_processed, y_train)\n",
    "\n",
    "    # 6. Evaluate\n",
    "    y_prob_val = model.predict_proba(X_val_processed)\n",
    "    y_pred_val = model.predict(X_val_processed, threshold=0.5)\n",
    "    \n",
    "    auc = roc_auc_score(y_val, y_prob_val)\n",
    "    print(f\"\\nValidation ROC AUC: {auc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "    # 7. Generate Submission\n",
    "    final_probs = model.predict_proba(X_test_processed)\n",
    "    submission = pd.DataFrame({'ID': test_ids, 'is_fraud': final_probs})\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved to submission.csv\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
