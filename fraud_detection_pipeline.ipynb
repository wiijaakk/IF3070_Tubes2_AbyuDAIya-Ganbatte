{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72073a3",
   "metadata": {},
   "source": [
    "# ðŸ” Fraud Detection with Logistic Regression from Scratch\n",
    "\n",
    "## Tugas Besar 2 IF3070 â€“ Dasar Inteligensi Artifisial\n",
    "\n",
    "**Author:** AbyuDAIya-Ganbatte Team\n",
    "\n",
    "This notebook implements a complete fraud detection pipeline using:\n",
    "- **Logistic Regression** implemented from scratch with NumPy\n",
    "- **Adam Optimizer** for efficient gradient descent\n",
    "- **KNN Imputation** for handling missing values\n",
    "- **Feature Engineering** with domain-specific transformations\n",
    "- **Elastic Net Regularization** (L1 + L2) for preventing overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Import Libraries](#1-import-libraries)\n",
    "2. [Load Data](#2-load-data)\n",
    "3. [Data Exploration](#3-data-exploration)\n",
    "4. [Preprocessing Pipeline](#4-preprocessing)\n",
    "   - Separate Features and Target\n",
    "   - KNN Imputation\n",
    "   - Feature Engineering (Ratio, Log, Interaction Features)\n",
    "   - One-Hot Encoding\n",
    "   - Outlier Clipping\n",
    "   - Standardization\n",
    "5. [Logistic Regression Algorithm](#5-logistic-regression)\n",
    "6. [Model Training & Evaluation](#6-training)\n",
    "7. [Generate Submission](#7-submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a18c0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We use only `numpy` and `pandas` for the core implementation, with `KNNImputer` from sklearn for handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fd93083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "NumPy version: 2.3.3\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b23dd",
   "metadata": {},
   "source": [
    "## 2. Load Training and Test Data\n",
    "\n",
    "Load the fraud detection datasets from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37631c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA LOADED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "Training data shape: (100000, 31)\n",
      "Test data shape: (100000, 30)\n",
      "\n",
      "Training columns: ['ID', 'transaction_id', 'user_id', 'age', 'gender', 'country', 'device_type', 'device_os', 'merchant_category', 'transaction_amount', 'transaction_type', 'time_of_day', 'day_of_week', 'transaction_duration', 'num_prev_transactions', 'avg_transaction_amount', 'std_transaction_amount', 'transactions_last_24h', 'transactions_last_1h', 'failed_login_attempts', 'ip_risk_score', 'device_trust_score', 'account_age_days', 'has_chargeback_history', 'shared_ip_users', 'shared_device_users', 'merchant_risk', 'country_risk', 'distance_from_home', 'is_new_country', 'is_fraud']\n",
      "\n",
      "Test columns: ['ID', 'transaction_id', 'user_id', 'age', 'gender', 'country', 'device_type', 'device_os', 'merchant_category', 'transaction_amount', 'transaction_type', 'time_of_day', 'day_of_week', 'transaction_duration', 'num_prev_transactions', 'avg_transaction_amount', 'std_transaction_amount', 'transactions_last_24h', 'transactions_last_1h', 'failed_login_attempts', 'ip_risk_score', 'device_trust_score', 'account_age_days', 'has_chargeback_history', 'shared_ip_users', 'shared_device_users', 'merchant_risk', 'country_risk', 'distance_from_home', 'is_new_country']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nTraining columns: {list(train_df.columns)}\")\n",
    "print(f\"\\nTest columns: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41bf96",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration\n",
    "\n",
    "Explore the data to understand fraud rate, missing values, and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9ef5967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TARGET DISTRIBUTION\n",
      "============================================================\n",
      "Fraud rate: 0.1413 (14.13%)\n",
      "Non-fraud: 85.87%\n",
      "\n",
      "Class counts:\n",
      "is_fraud\n",
      "0    85872\n",
      "1    14128\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "MISSING VALUES\n",
      "============================================================\n",
      "\n",
      "Training data missing values:\n",
      "transaction_amount        2419\n",
      "avg_transaction_amount    2419\n",
      "device_trust_score        2419\n",
      "dtype: int64\n",
      "\n",
      "Total missing in train: 7257\n",
      "Total missing in test: 7212\n",
      "\n",
      "============================================================\n",
      "DATA TYPES\n",
      "============================================================\n",
      "ID                          int64\n",
      "transaction_id             object\n",
      "user_id                     int64\n",
      "age                         int64\n",
      "gender                     object\n",
      "country                    object\n",
      "device_type                object\n",
      "device_os                  object\n",
      "merchant_category          object\n",
      "transaction_amount        float64\n",
      "transaction_type           object\n",
      "time_of_day                 int64\n",
      "day_of_week                 int64\n",
      "transaction_duration      float64\n",
      "num_prev_transactions       int64\n",
      "avg_transaction_amount    float64\n",
      "std_transaction_amount    float64\n",
      "transactions_last_24h       int64\n",
      "transactions_last_1h        int64\n",
      "failed_login_attempts       int64\n",
      "ip_risk_score             float64\n",
      "device_trust_score        float64\n",
      "account_age_days            int64\n",
      "has_chargeback_history      int64\n",
      "shared_ip_users             int64\n",
      "shared_device_users         int64\n",
      "merchant_risk             float64\n",
      "country_risk              float64\n",
      "distance_from_home        float64\n",
      "is_new_country              int64\n",
      "is_fraud                    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check fraud rate (target distribution)\n",
    "fraud_rate = train_df['is_fraud'].mean()\n",
    "print(\"=\" * 60)\n",
    "print(\"TARGET DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Fraud rate: {fraud_rate:.4f} ({fraud_rate*100:.2f}%)\")\n",
    "print(f\"Non-fraud: {(1-fraud_rate)*100:.2f}%\")\n",
    "print(f\"\\nClass counts:\")\n",
    "print(train_df['is_fraud'].value_counts())\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "missing_train = train_df.isnull().sum()\n",
    "missing_test = test_df.isnull().sum()\n",
    "print(\"\\nTraining data missing values:\")\n",
    "print(missing_train[missing_train > 0])\n",
    "print(f\"\\nTotal missing in train: {missing_train.sum()}\")\n",
    "print(f\"Total missing in test: {missing_test.sum()}\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\" * 60)\n",
    "print(train_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10097dca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Preprocessing Pipeline\n",
    "\n",
    "The preprocessing pipeline is implemented sequentially (without functions) for clarity.\n",
    "\n",
    "## 4.1 Separate Features and Target\n",
    "\n",
    "Extract the target variable and prepare feature matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "669407a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURES SEPARATED\n",
      "============================================================\n",
      "\n",
      "Numerical columns (21):\n",
      "['age', 'transaction_amount', 'time_of_day', 'day_of_week', 'transaction_duration', 'num_prev_transactions', 'avg_transaction_amount', 'std_transaction_amount', 'transactions_last_24h', 'transactions_last_1h', 'failed_login_attempts', 'ip_risk_score', 'device_trust_score', 'account_age_days', 'has_chargeback_history', 'shared_ip_users', 'shared_device_users', 'merchant_risk', 'country_risk', 'distance_from_home', 'is_new_country']\n",
      "\n",
      "Categorical columns (6):\n",
      "['gender', 'country', 'device_type', 'device_os', 'merchant_category', 'transaction_type']\n",
      "\n",
      "Training features shape: (100000, 27)\n",
      "Test features shape: (100000, 27)\n"
     ]
    }
   ],
   "source": [
    "# Store test IDs for submission\n",
    "test_ids = test_df['ID'].values\n",
    "\n",
    "# Extract target variable\n",
    "y_train = train_df['is_fraud'].values\n",
    "\n",
    "# Columns to drop (IDs are not useful as features)\n",
    "cols_to_drop = ['ID', 'transaction_id', 'user_id']\n",
    "\n",
    "# Create feature DataFrames\n",
    "train_features = train_df.drop(columns=['is_fraud'] + cols_to_drop, errors='ignore')\n",
    "test_features = test_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Identify column types\n",
    "numerical_cols = train_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train_features.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURES SEPARATED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}):\")\n",
    "print(numerical_cols)\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}):\")\n",
    "print(categorical_cols)\n",
    "print(f\"\\nTraining features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e157a0",
   "metadata": {},
   "source": [
    "## 4.2 Handle Missing Values with KNN Imputation\n",
    "\n",
    "**KNN Imputation** uses the k-nearest neighbors algorithm to impute missing values based on the similarity of other features. This is more sophisticated than simple mean/mode imputation as it considers the local structure of the data.\n",
    "\n",
    "- **Numerical columns**: KNN imputation with distance-weighted neighbors (k=5)\n",
    "- **Categorical columns**: Mode imputation (most frequent value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a6da7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying KNN imputation (k=5) for numerical columns...\n",
      "âœ“ Imputed 7257 missing values in training data\n",
      "\n",
      "Applying mode imputation for categorical columns...\n",
      "\n",
      "âœ“ Missing value handling complete for training data\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# KNN IMPUTATION FOR NUMERICAL COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "KNN_NEIGHBORS = 5  # Number of neighbors for KNN imputation\n",
    "imputation_info = {}  # Store imputation info for test data\n",
    "\n",
    "# Get numerical columns that exist in both train and test\n",
    "numerical_cols_in_df = [col for col in numerical_cols if col in train_features.columns]\n",
    "\n",
    "# Check if there are any missing values in numerical columns\n",
    "has_missing_numerical = train_features[numerical_cols_in_df].isnull().any().any()\n",
    "\n",
    "if has_missing_numerical:\n",
    "    print(f\"Applying KNN imputation (k={KNN_NEIGHBORS}) for numerical columns...\")\n",
    "    \n",
    "    # Create and fit KNN imputer\n",
    "    knn_imputer = KNNImputer(n_neighbors=KNN_NEIGHBORS, weights='distance')\n",
    "    \n",
    "    # Fit on training data and transform\n",
    "    numerical_data_train = train_features[numerical_cols_in_df].values\n",
    "    imputed_train = knn_imputer.fit_transform(numerical_data_train)\n",
    "    \n",
    "    # Update training DataFrame\n",
    "    train_features[numerical_cols_in_df] = imputed_train\n",
    "    \n",
    "    # Store imputer for test data\n",
    "    imputation_info['knn_imputer'] = knn_imputer\n",
    "    imputation_info['numerical_cols'] = numerical_cols_in_df\n",
    "    \n",
    "    print(f\"âœ“ Imputed {train_df[numerical_cols_in_df].isnull().sum().sum()} missing values in training data\")\n",
    "else:\n",
    "    print(\"No missing values in numerical columns\")\n",
    "    imputation_info['knn_imputer'] = None\n",
    "    imputation_info['numerical_cols'] = numerical_cols_in_df\n",
    "\n",
    "# ============================================================================\n",
    "# MODE IMPUTATION FOR CATEGORICAL COLUMNS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nApplying mode imputation for categorical columns...\")\n",
    "mode_values = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in train_features.columns and train_features[col].isnull().any():\n",
    "        # Get the mode (most frequent value)\n",
    "        mode_value = train_features[col].mode()\n",
    "        if len(mode_value) > 0:\n",
    "            mode_value = mode_value[0]\n",
    "            train_features[col] = train_features[col].fillna(mode_value)\n",
    "            mode_values[col] = mode_value\n",
    "            print(f\"  âœ“ {col}: filled with '{mode_value}'\")\n",
    "\n",
    "imputation_info['mode_values'] = mode_values\n",
    "\n",
    "print(\"\\nâœ“ Missing value handling complete for training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d130fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying imputation to test data...\n",
      "âœ“ KNN imputation applied to test numerical columns\n",
      "\n",
      "âœ“ Imputation complete for both train and test data\n",
      "Remaining missing in train: 0\n",
      "Remaining missing in test: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# APPLY IMPUTATION TO TEST DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Applying imputation to test data...\")\n",
    "\n",
    "# Apply KNN imputation for numerical columns\n",
    "if imputation_info['knn_imputer'] is not None:\n",
    "    knn_imputer = imputation_info['knn_imputer']\n",
    "    numerical_cols_imp = imputation_info['numerical_cols']\n",
    "    \n",
    "    cols_to_impute = [col for col in numerical_cols_imp if col in test_features.columns]\n",
    "    \n",
    "    if len(cols_to_impute) > 0 and test_features[cols_to_impute].isnull().any().any():\n",
    "        numerical_data_test = test_features[cols_to_impute].values\n",
    "        imputed_test = knn_imputer.transform(numerical_data_test)\n",
    "        test_features[cols_to_impute] = imputed_test\n",
    "        print(f\"âœ“ KNN imputation applied to test numerical columns\")\n",
    "\n",
    "# Apply mode imputation for categorical columns\n",
    "for col, mode_val in imputation_info['mode_values'].items():\n",
    "    if col in test_features.columns and test_features[col].isnull().any():\n",
    "        test_features[col] = test_features[col].fillna(mode_val)\n",
    "        print(f\"âœ“ Mode imputation applied to test column: {col}\")\n",
    "\n",
    "# Handle any remaining missing values in test\n",
    "for col in numerical_cols:\n",
    "    if col in test_features.columns and test_features[col].isnull().any():\n",
    "        test_features[col] = test_features[col].fillna(0)\n",
    "        \n",
    "for col in categorical_cols:\n",
    "    if col in test_features.columns and test_features[col].isnull().any():\n",
    "        mode_val = train_features[col].mode()[0] if len(train_features[col].mode()) > 0 else 'unknown'\n",
    "        test_features[col] = test_features[col].fillna(mode_val)\n",
    "\n",
    "print(\"\\nâœ“ Imputation complete for both train and test data\")\n",
    "print(f\"Remaining missing in train: {train_features.isnull().sum().sum()}\")\n",
    "print(f\"Remaining missing in test: {test_features.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d23c5",
   "metadata": {},
   "source": [
    "## 4.3 Feature Engineering: Ratio Features\n",
    "\n",
    "Create meaningful ratio features that capture relationships between existing features. These ratios can help detect anomalous transaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "446a4bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ratio features...\n",
      "  âœ“ Created: amount_vs_avg_ratio\n",
      "  âœ“ Created: amount_vs_std_ratio\n",
      "  âœ“ Created: hourly_vs_daily_ratio\n",
      "  âœ“ Created: failed_vs_total_ratio\n",
      "  âœ“ Created: ip_vs_device_shared_ratio\n",
      "\n",
      "âœ“ Ratio features created\n",
      "Current train shape: (100000, 32)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE RATIO FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Creating ratio features...\")\n",
    "\n",
    "# Define meaningful ratio pairs for fraud detection\n",
    "# (numerator, denominator, new_feature_name)\n",
    "ratio_pairs = [\n",
    "    ('transaction_amount', 'avg_transaction_amount', 'amount_vs_avg_ratio'),\n",
    "    ('transaction_amount', 'std_transaction_amount', 'amount_vs_std_ratio'),\n",
    "    ('transactions_last_1h', 'transactions_last_24h', 'hourly_vs_daily_ratio'),\n",
    "    ('failed_login_attempts', 'num_prev_transactions', 'failed_vs_total_ratio'),\n",
    "    ('shared_ip_users', 'shared_device_users', 'ip_vs_device_shared_ratio'),\n",
    "]\n",
    "\n",
    "for num, denom, name in ratio_pairs:\n",
    "    if num in train_features.columns and denom in train_features.columns:\n",
    "        # Avoid division by zero by replacing 0 with small value\n",
    "        denom_safe_train = train_features[denom].replace(0, 1e-10)\n",
    "        denom_safe_test = test_features[denom].replace(0, 1e-10)\n",
    "        \n",
    "        train_features[name] = train_features[num] / denom_safe_train\n",
    "        test_features[name] = test_features[num] / denom_safe_test\n",
    "        print(f\"  âœ“ Created: {name}\")\n",
    "\n",
    "print(f\"\\nâœ“ Ratio features created\")\n",
    "print(f\"Current train shape: {train_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c1cb07",
   "metadata": {},
   "source": [
    "## 4.4 Feature Engineering: Log-Transformed Features\n",
    "\n",
    "Apply log transformation to skewed numerical features. This helps normalize distributions and can improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72508118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating log-transformed features...\n",
      "  âœ“ Created: transaction_amount_log\n",
      "  âœ“ Created: std_transaction_amount_log\n",
      "  âœ“ Created: account_age_days_log\n",
      "  âœ“ Created: distance_from_home_log\n",
      "  âœ“ Created: num_prev_transactions_log\n",
      "\n",
      "âœ“ Log features created\n",
      "Current train shape: (100000, 37)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE LOG-TRANSFORMED FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Creating log-transformed features...\")\n",
    "\n",
    "# Columns that typically benefit from log transformation (skewed distributions)\n",
    "log_candidates = [\n",
    "    'transaction_amount', \n",
    "    'avg_transaction_amount', \n",
    "    'std_transaction_amount',\n",
    "    'account_age_days', \n",
    "    'distance_from_home', \n",
    "    'num_prev_transactions'\n",
    "]\n",
    "\n",
    "for col in log_candidates:\n",
    "    if col in train_features.columns:\n",
    "        # Check if column has non-negative values\n",
    "        min_val_train = train_features[col].min()\n",
    "        min_val_test = test_features[col].min()\n",
    "        \n",
    "        if min_val_train >= 0 and min_val_test >= 0:\n",
    "            # Use log1p (log(1+x)) for numerical stability\n",
    "            train_features[f'{col}_log'] = np.log1p(train_features[col])\n",
    "            test_features[f'{col}_log'] = np.log1p(test_features[col])\n",
    "            print(f\"  âœ“ Created: {col}_log\")\n",
    "\n",
    "print(f\"\\nâœ“ Log features created\")\n",
    "print(f\"Current train shape: {train_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21f7d5",
   "metadata": {},
   "source": [
    "## 4.5 Feature Engineering: Interaction Features\n",
    "\n",
    "Create domain-specific interaction features that capture complex patterns in fraud behavior. These features combine multiple signals to create more informative predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ee70c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction features...\n",
      "  âœ“ risk_interaction\n",
      "  âœ“ merchant_country_risk\n",
      "  âœ“ amount_zscore\n",
      "  âœ“ hourly_concentration\n",
      "  âœ“ tx_per_day_age\n",
      "  âœ“ new_location_distance\n",
      "  âœ“ failed_login_amount\n",
      "  âœ“ total_shared_users, shared_resource_product\n",
      "  âœ“ is_night_time\n",
      "  âœ“ is_weekend\n",
      "  âœ“ chargeback_high_amount\n",
      "  âœ“ high_risk_high_amount\n",
      "  âœ“ failed_login_risk\n",
      "  âœ“ new_country_amount\n",
      "  âœ“ ip_risk_score_squared\n",
      "  âœ“ merchant_risk_squared\n",
      "  âœ“ country_risk_squared\n",
      "  âœ“ combined_risk\n",
      "\n",
      "âœ“ Interaction features created\n",
      "Current train shape: (100000, 56)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CREATE INTERACTION FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# --- Risk Score Interactions ---\n",
    "if 'ip_risk_score' in train_features.columns and 'device_trust_score' in train_features.columns:\n",
    "    # Combined risk indicator (high IP risk + low device trust)\n",
    "    train_features['risk_interaction'] = train_features['ip_risk_score'] * (1 - train_features['device_trust_score'] / 100)\n",
    "    test_features['risk_interaction'] = test_features['ip_risk_score'] * (1 - test_features['device_trust_score'] / 100)\n",
    "    print(\"  âœ“ risk_interaction\")\n",
    "\n",
    "if 'merchant_risk' in train_features.columns and 'country_risk' in train_features.columns:\n",
    "    # Combined merchant-country risk\n",
    "    train_features['merchant_country_risk'] = train_features['merchant_risk'] * train_features['country_risk']\n",
    "    test_features['merchant_country_risk'] = test_features['merchant_risk'] * test_features['country_risk']\n",
    "    print(\"  âœ“ merchant_country_risk\")\n",
    "\n",
    "# --- Transaction Amount Anomaly ---\n",
    "if 'transaction_amount' in train_features.columns and 'avg_transaction_amount' in train_features.columns:\n",
    "    if 'std_transaction_amount' in train_features.columns:\n",
    "        # Z-score like feature\n",
    "        train_features['amount_zscore'] = (train_features['transaction_amount'] - train_features['avg_transaction_amount']) / (train_features['std_transaction_amount'] + 1e-6)\n",
    "        test_features['amount_zscore'] = (test_features['transaction_amount'] - test_features['avg_transaction_amount']) / (test_features['std_transaction_amount'] + 1e-6)\n",
    "        print(\"  âœ“ amount_zscore\")\n",
    "\n",
    "# --- Velocity-based Features ---\n",
    "if 'transactions_last_24h' in train_features.columns and 'transactions_last_1h' in train_features.columns:\n",
    "    # Recent activity concentration\n",
    "    train_features['hourly_concentration'] = train_features['transactions_last_1h'] / (train_features['transactions_last_24h'] + 1)\n",
    "    test_features['hourly_concentration'] = test_features['transactions_last_1h'] / (test_features['transactions_last_24h'] + 1)\n",
    "    print(\"  âœ“ hourly_concentration\")\n",
    "\n",
    "# --- Account Trust Features ---\n",
    "if 'account_age_days' in train_features.columns and 'num_prev_transactions' in train_features.columns:\n",
    "    # Transaction frequency per day of account age\n",
    "    train_features['tx_per_day_age'] = train_features['num_prev_transactions'] / (train_features['account_age_days'] + 1)\n",
    "    test_features['tx_per_day_age'] = test_features['num_prev_transactions'] / (test_features['account_age_days'] + 1)\n",
    "    print(\"  âœ“ tx_per_day_age\")\n",
    "\n",
    "# --- Location Risk ---\n",
    "if 'is_new_country' in train_features.columns and 'distance_from_home' in train_features.columns:\n",
    "    train_features['new_location_distance'] = train_features['is_new_country'] * train_features['distance_from_home']\n",
    "    test_features['new_location_distance'] = test_features['is_new_country'] * test_features['distance_from_home']\n",
    "    print(\"  âœ“ new_location_distance\")\n",
    "\n",
    "# --- Failed Login Impact ---\n",
    "if 'failed_login_attempts' in train_features.columns and 'transaction_amount' in train_features.columns:\n",
    "    train_features['failed_login_amount'] = train_features['failed_login_attempts'] * train_features['transaction_amount']\n",
    "    test_features['failed_login_amount'] = test_features['failed_login_attempts'] * test_features['transaction_amount']\n",
    "    print(\"  âœ“ failed_login_amount\")\n",
    "\n",
    "# --- Shared Resource Risk ---\n",
    "if 'shared_ip_users' in train_features.columns and 'shared_device_users' in train_features.columns:\n",
    "    train_features['total_shared_users'] = train_features['shared_ip_users'] + train_features['shared_device_users']\n",
    "    test_features['total_shared_users'] = test_features['shared_ip_users'] + test_features['shared_device_users']\n",
    "    \n",
    "    train_features['shared_resource_product'] = train_features['shared_ip_users'] * train_features['shared_device_users']\n",
    "    test_features['shared_resource_product'] = test_features['shared_ip_users'] * test_features['shared_device_users']\n",
    "    print(\"  âœ“ total_shared_users, shared_resource_product\")\n",
    "\n",
    "# --- Time-based Risk ---\n",
    "if 'time_of_day' in train_features.columns:\n",
    "    # Night time risk (0-6 AM and 22-24)\n",
    "    train_features['is_night_time'] = ((train_features['time_of_day'] >= 0) & (train_features['time_of_day'] <= 6) | \n",
    "                                       (train_features['time_of_day'] >= 22)).astype(float)\n",
    "    test_features['is_night_time'] = ((test_features['time_of_day'] >= 0) & (test_features['time_of_day'] <= 6) | \n",
    "                                      (test_features['time_of_day'] >= 22)).astype(float)\n",
    "    print(\"  âœ“ is_night_time\")\n",
    "\n",
    "if 'day_of_week' in train_features.columns:\n",
    "    # Weekend indicator\n",
    "    train_features['is_weekend'] = (train_features['day_of_week'] >= 5).astype(float)\n",
    "    test_features['is_weekend'] = (test_features['day_of_week'] >= 5).astype(float)\n",
    "    print(\"  âœ“ is_weekend\")\n",
    "\n",
    "# --- Chargeback History Interaction ---\n",
    "if 'has_chargeback_history' in train_features.columns and 'transaction_amount' in train_features.columns:\n",
    "    median_amount = train_features['transaction_amount'].median()\n",
    "    train_features['chargeback_high_amount'] = train_features['has_chargeback_history'] * (train_features['transaction_amount'] > median_amount).astype(float)\n",
    "    test_features['chargeback_high_amount'] = test_features['has_chargeback_history'] * (test_features['transaction_amount'] > median_amount).astype(float)\n",
    "    print(\"  âœ“ chargeback_high_amount\")\n",
    "\n",
    "# --- Additional High-Value Interactions ---\n",
    "if 'ip_risk_score' in train_features.columns and 'transaction_amount' in train_features.columns:\n",
    "    train_features['high_risk_high_amount'] = train_features['ip_risk_score'] * train_features['transaction_amount']\n",
    "    test_features['high_risk_high_amount'] = test_features['ip_risk_score'] * test_features['transaction_amount']\n",
    "    print(\"  âœ“ high_risk_high_amount\")\n",
    "\n",
    "if 'failed_login_attempts' in train_features.columns and 'ip_risk_score' in train_features.columns:\n",
    "    train_features['failed_login_risk'] = train_features['failed_login_attempts'] * train_features['ip_risk_score']\n",
    "    test_features['failed_login_risk'] = test_features['failed_login_attempts'] * test_features['ip_risk_score']\n",
    "    print(\"  âœ“ failed_login_risk\")\n",
    "\n",
    "if 'is_new_country' in train_features.columns and 'transaction_amount' in train_features.columns:\n",
    "    train_features['new_country_amount'] = train_features['is_new_country'] * train_features['transaction_amount']\n",
    "    test_features['new_country_amount'] = test_features['is_new_country'] * test_features['transaction_amount']\n",
    "    print(\"  âœ“ new_country_amount\")\n",
    "\n",
    "# --- Squared Features for Important Risk Indicators ---\n",
    "for col in ['ip_risk_score', 'merchant_risk', 'country_risk']:\n",
    "    if col in train_features.columns:\n",
    "        train_features[f'{col}_squared'] = train_features[col] ** 2\n",
    "        test_features[f'{col}_squared'] = test_features[col] ** 2\n",
    "        print(f\"  âœ“ {col}_squared\")\n",
    "\n",
    "# --- Combined Risk Score ---\n",
    "risk_cols = []\n",
    "for col in ['ip_risk_score', 'merchant_risk', 'country_risk']:\n",
    "    if col in train_features.columns:\n",
    "        risk_cols.append(col)\n",
    "\n",
    "if len(risk_cols) > 0:\n",
    "    train_features['combined_risk'] = train_features[risk_cols].mean(axis=1)\n",
    "    test_features['combined_risk'] = test_features[risk_cols].mean(axis=1)\n",
    "    print(\"  âœ“ combined_risk\")\n",
    "\n",
    "print(f\"\\nâœ“ Interaction features created\")\n",
    "print(f\"Current train shape: {train_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f9747",
   "metadata": {},
   "source": [
    "## 4.6 One-Hot Encode Categorical Features\n",
    "\n",
    "Convert categorical variables into binary dummy variables using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a6ad825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying one-hot encoding to categorical features...\n",
      "  âœ“ gender: 2 categories encoded\n",
      "  âœ“ country: 10 categories encoded\n",
      "  âœ“ device_type: 3 categories encoded\n",
      "  âœ“ device_os: 5 categories encoded\n",
      "  âœ“ merchant_category: 9 categories encoded\n",
      "  âœ“ transaction_type: 4 categories encoded\n",
      "\n",
      "âœ“ One-hot encoding complete\n",
      "Train shape after encoding: (100000, 83)\n",
      "Test shape after encoding: (100000, 83)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ONE-HOT ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Applying one-hot encoding to categorical features...\")\n",
    "\n",
    "encoding_info = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in train_features.columns:\n",
    "        # Get unique categories from training data (sorted for consistency)\n",
    "        categories = train_features[col].unique()\n",
    "        categories = [c for c in categories if pd.notna(c)]\n",
    "        categories = sorted(categories)\n",
    "        \n",
    "        encoding_info[col] = categories\n",
    "        \n",
    "        # Create dummy columns for each category\n",
    "        for category in categories:\n",
    "            new_col_name = f\"{col}_{category}\"\n",
    "            train_features[new_col_name] = (train_features[col] == category).astype(int)\n",
    "            test_features[new_col_name] = (test_features[col] == category).astype(int)\n",
    "        \n",
    "        # Drop original categorical column\n",
    "        train_features = train_features.drop(columns=[col])\n",
    "        test_features = test_features.drop(columns=[col])\n",
    "        \n",
    "        print(f\"  âœ“ {col}: {len(categories)} categories encoded\")\n",
    "\n",
    "print(f\"\\nâœ“ One-hot encoding complete\")\n",
    "print(f\"Train shape after encoding: {train_features.shape}\")\n",
    "print(f\"Test shape after encoding: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed1733",
   "metadata": {},
   "source": [
    "## 4.7 Align Train and Test Columns\n",
    "\n",
    "Ensure both datasets have the same columns after all transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4998f094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning train and test columns...\n",
      "\n",
      "âœ“ Columns aligned\n",
      "Final number of features: 83\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ALIGN COLUMNS BETWEEN TRAIN AND TEST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Aligning train and test columns...\")\n",
    "\n",
    "train_cols = set(train_features.columns)\n",
    "test_cols = set(test_features.columns)\n",
    "\n",
    "# Add missing columns to test (with zeros)\n",
    "missing_in_test = train_cols - test_cols\n",
    "for col in missing_in_test:\n",
    "    test_features[col] = 0\n",
    "if missing_in_test:\n",
    "    print(f\"  Added {len(missing_in_test)} missing columns to test data\")\n",
    "\n",
    "# Add missing columns to train (shouldn't happen often)\n",
    "missing_in_train = test_cols - train_cols\n",
    "for col in missing_in_train:\n",
    "    train_features[col] = 0\n",
    "if missing_in_train:\n",
    "    print(f\"  Added {len(missing_in_train)} missing columns to train data\")\n",
    "\n",
    "# Ensure same column order (sorted)\n",
    "all_cols = sorted(train_features.columns.tolist())\n",
    "train_features = train_features[all_cols]\n",
    "test_features = test_features[all_cols]\n",
    "\n",
    "print(f\"\\nâœ“ Columns aligned\")\n",
    "print(f\"Final number of features: {len(all_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f0e42",
   "metadata": {},
   "source": [
    "## 4.8 Clip Outliers\n",
    "\n",
    "Apply percentile-based outlier clipping (winsorization) to limit extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd1be84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipping outliers (1st to 99th percentile)...\n",
      "\n",
      "âœ“ Outlier clipping complete\n",
      "Train shape after clipping: (100000, 83)\n",
      "Test shape after clipping: (100000, 83)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLIP OUTLIERS (WINSORIZATION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Clipping outliers (1st to 99th percentile)...\")\n",
    "\n",
    "train_values = train_features.values\n",
    "\n",
    "def clip_outliers(X, lower_percentile=1, upper_percentile=99):\n",
    "    X = X.copy()\n",
    "    n_features = X.shape[1]\n",
    "    clip_bounds = {'lower': [], 'upper': []}\n",
    "    for i in range(n_features):\n",
    "        lower = np.percentile(X[:, i], lower_percentile)\n",
    "        upper = np.percentile(X[:, i], upper_percentile)\n",
    "        clip_bounds['lower'].append(lower)\n",
    "        clip_bounds['upper'].append(upper)\n",
    "        X[:, i] = np.clip(X[:, i], lower, upper)\n",
    "    return X, clip_bounds\n",
    "\n",
    "def apply_clip_outliers(X, clip_bounds):\n",
    "    X = X.copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        X[:, i] = np.clip(X[:, i], clip_bounds['lower'][i], clip_bounds['upper'][i])\n",
    "    return X\n",
    "\n",
    "train_clipped, clip_bounds = clip_outliers(train_values, lower_percentile=1, upper_percentile=99)\n",
    "test_clipped = apply_clip_outliers(test_features.values, clip_bounds)\n",
    "\n",
    "print(f\"\\nâœ“ Outlier clipping complete\")\n",
    "print(f\"Train shape after clipping: {train_clipped.shape}\")\n",
    "print(f\"Test shape after clipping: {test_clipped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbb4a8",
   "metadata": {},
   "source": [
    "## 4.9 Standardize Features\n",
    "\n",
    "Standardize features to zero mean and unit variance using a custom StandardScaler (from scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88119e47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Standardize features (zero mean, unit variance) without using sklearn\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Fit scaler on train, apply to both train and test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m means = \u001b[43mX_train\u001b[49m.mean(axis=\u001b[32m0\u001b[39m)\n\u001b[32m      5\u001b[39m stds = X_train.std(axis=\u001b[32m0\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Avoid division by zero\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Standardize features (zero mean, unit variance) without using sklearn\n",
    "# Fit scaler on train, apply to both train and test\n",
    "\n",
    "means = X_train.mean(axis=0)\n",
    "stds = X_train.std(axis=0)\n",
    "\n",
    "# Avoid division by zero\n",
    "stds_replaced = stds.replace(0, 1)\n",
    "\n",
    "X_train_scaled = (X_train - means) / stds_replaced\n",
    "X_test_scaled = (X_test - means) / stds_replaced\n",
    "\n",
    "print(\"Standardization complete.\")\n",
    "print(f\"Train shape: {X_train_scaled.shape}, Test shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd74b6",
   "metadata": {},
   "source": [
    "## 5. Train/Validation Split\n",
    "\n",
    "Split the training data into train and validation sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data into train/validation sets (80/20 split, stratified)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_tr.shape}, Validation set: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcb453",
   "metadata": {},
   "source": [
    "## 6. Metrics and Helper Functions\n",
    "\n",
    "Define evaluation metrics (AUC, accuracy, etc.) and helper functions for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict_proba(X, w, b):\n",
    "    return sigmoid(np.dot(X, w) + b)\n",
    "\n",
    "def predict(X, w, b, threshold=0.5):\n",
    "    return (predict_proba(X, w, b) >= threshold).astype(int)\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred_proba, y_pred):\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"AUC: {auc:.4f}, Accuracy: {acc:.4f}\")\n",
    "    return auc, acc\n",
    "\n",
    "print(\"Metrics and helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be095fb8",
   "metadata": {},
   "source": [
    "## 7. Logistic Regression Model (from Scratch)\n",
    "\n",
    "Define the custom Logistic Regression class with Adam optimizer, elastic net regularization, and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7959da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, lr=0.01, n_iter=1000, batch_size=64, l1=0.0, l2=0.0, beta1=0.9, beta2=0.999, eps=1e-8, early_stopping_rounds=20, verbose=True):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.verbose = verbose\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.best_weights = None\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.no_improve_rounds = 0\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _loss(self, y, y_pred):\n",
    "        # Binary cross-entropy + elastic net\n",
    "        eps = 1e-8\n",
    "        ce = -np.mean(y * np.log(y_pred + eps) + (1 - y) * np.log(1 - y_pred + eps))\n",
    "        l1_penalty = self.l1 * np.sum(np.abs(self.w))\n",
    "        l2_penalty = self.l2 * np.sum(self.w ** 2)\n",
    "        return ce + l1_penalty + l2_penalty\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0.0\n",
    "        m_w, v_w = np.zeros(n_features), np.zeros(n_features)\n",
    "        m_b, v_b = 0.0, 0.0\n",
    "        t = 0\n",
    "        for epoch in range(self.n_iter):\n",
    "            # Shuffle\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            X_shuf, y_shuf = X[idx], y[idx]\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = min(start + self.batch_size, n_samples)\n",
    "                X_batch, y_batch = X_shuf[start:end], y_shuf[start:end]\n",
    "                y_pred = self._sigmoid(np.dot(X_batch, self.w) + self.b)\n",
    "                error = y_pred - y_batch\n",
    "                grad_w = np.dot(X_batch.T, error) / len(y_batch) + self.l1 * np.sign(self.w) + 2 * self.l2 * self.w\n",
    "                grad_b = np.mean(error)\n",
    "                t += 1\n",
    "                # Adam update\n",
    "                m_w = self.beta1 * m_w + (1 - self.beta1) * grad_w\n",
    "                v_w = self.beta2 * v_w + (1 - self.beta2) * (grad_w ** 2)\n",
    "                m_w_hat = m_w / (1 - self.beta1 ** t)\n",
    "                v_w_hat = v_w / (1 - self.beta2 ** t)\n",
    "                self.w -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.eps)\n",
    "                m_b = self.beta1 * m_b + (1 - self.beta1) * grad_b\n",
    "                v_b = self.beta2 * v_b + (1 - self.beta2) * (grad_b ** 2)\n",
    "                m_b_hat = m_b / (1 - self.beta1 ** t)\n",
    "                v_b_hat = v_b / (1 - self.beta2 ** t)\n",
    "                self.b -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.eps)\n",
    "            # Track loss\n",
    "            y_pred_train = self._sigmoid(np.dot(X, self.w) + self.b)\n",
    "            train_loss = self._loss(y, y_pred_train)\n",
    "            self.loss_history.append(train_loss)\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self._sigmoid(np.dot(X_val, self.w) + self.b)\n",
    "                val_loss = self._loss(y_val, y_pred_val)\n",
    "                self.val_loss_history.append(val_loss)\n",
    "                if self.verbose:\n",
    "                    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "                # Early stopping\n",
    "                if val_loss < self.best_val_loss - 1e-5:\n",
    "                    self.best_val_loss = val_loss\n",
    "                    self.best_weights = (self.w.copy(), self.b)\n",
    "                    self.no_improve_rounds = 0\n",
    "                else:\n",
    "                    self.no_improve_rounds += 1\n",
    "                if self.no_improve_rounds >= self.early_stopping_rounds:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    if self.best_weights is not None:\n",
    "                        self.w, self.b = self.best_weights\n",
    "                    return\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}\")\n",
    "        # Restore best weights if early stopping\n",
    "        if self.best_weights is not None:\n",
    "            self.w, self.b = self.best_weights\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self._sigmoid(np.dot(X, self.w) + self.b)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "\n",
    "print(\"Custom Logistic Regression class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b2001",
   "metadata": {},
   "source": [
    "## 8. Model Training\n",
    "\n",
    "Train the custom logistic regression model on the training set and monitor validation loss for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eccfd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the model\n",
    "model = LogisticRegressionScratch(\n",
    "    lr=0.01, n_iter=200, batch_size=128, l1=1e-4, l2=1e-4, early_stopping_rounds=10, verbose=True\n",
    ")\n",
    "model.fit(X_tr.values, y_tr.values, X_val.values, y_val.values)\n",
    "\n",
    "print(\"Model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb84f72",
   "metadata": {},
   "source": [
    "## 9. Validation Performance and Threshold Tuning\n",
    "\n",
    "Evaluate the model on the validation set, tune the threshold for best AUC, and print metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cb637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set and tune threshold for best AUC\n",
    "val_proba = model.predict_proba(X_val.values)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "best_auc = 0\n",
    "best_threshold = 0.5\n",
    "for thresh in np.arange(0.1, 0.9, 0.01):\n",
    "    val_pred = (val_proba >= thresh).astype(int)\n",
    "    auc = roc_auc_score(y_val, val_proba)\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_threshold = thresh\n",
    "\n",
    "val_pred = (val_proba >= best_threshold).astype(int)\n",
    "auc, acc = evaluate_metrics(y_val.values, val_proba, val_pred)\n",
    "print(f\"Best threshold: {best_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22970b8",
   "metadata": {},
   "source": [
    "## 10. Predict on Test Set and Prepare Submission\n",
    "\n",
    "Use the trained model to predict probabilities on the test set, apply the best threshold, and prepare the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbb25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "proba_test = model.predict_proba(X_test_scaled.values)\n",
    "pred_test = (proba_test >= best_threshold).astype(int)\n",
    "\n",
    "# Prepare submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'label': pred_test\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509e953",
   "metadata": {},
   "source": [
    "## 11. Save Model and Scaler Parameters\n",
    "\n",
    "Save the trained model weights and scaler parameters for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ecc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save model weights\n",
    "model_params = {\n",
    "    'weights': model.w.tolist(),\n",
    "    'bias': float(model.b)\n",
    "}\n",
    "with open('logistic_model.json', 'w') as f:\n",
    "    json.dump(model_params, f)\n",
    "\n",
    "# Save scaler parameters\n",
    "scaler_params = {\n",
    "    'means': means.tolist(),\n",
    "    'stds': stds.tolist()\n",
    "}\n",
    "with open('scaler_params.json', 'w') as f:\n",
    "    json.dump(scaler_params, f)\n",
    "\n",
    "print(\"Model and scaler parameters saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9033cc54",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Pipeline Complete\n",
    "\n",
    "This notebook provides a fully transparent, step-by-step implementation of the fraud detection pipeline, with all preprocessing visible and only the model code encapsulated in a class. You can now run all cells to reproduce the results and generate a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CLIP OUTLIERS (WINSORIZATION)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Clipping outliers using percentile-based bounds...\")\n",
    "\n",
    "LOWER_PERCENTILE = 1\n",
    "UPPER_PERCENTILE = 99\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_values = train_features.values.copy()\n",
    "test_values = test_features.values.copy()\n",
    "\n",
    "n_features = train_values.shape[1]\n",
    "clip_bounds = {'lower': [], 'upper': []}\n",
    "\n",
    "# Compute clip bounds from training data and apply to both\n",
    "for i in range(n_features):\n",
    "    lower = np.percentile(train_values[:, i], LOWER_PERCENTILE)\n",
    "    upper = np.percentile(train_values[:, i], UPPER_PERCENTILE)\n",
    "    \n",
    "    clip_bounds['lower'].append(lower)\n",
    "    clip_bounds['upper'].append(upper)\n",
    "    \n",
    "    # Clip training data\n",
    "    train_values[:, i] = np.clip(train_values[:, i], lower, upper)\n",
    "    \n",
    "    # Clip test data using same bounds\n",
    "    test_values[:, i] = np.clip(test_values[:, i], lower, upper)\n",
    "\n",
    "print(f\"âœ“ Outliers clipped to [{LOWER_PERCENTILE}th, {UPPER_PERCENTILE}th] percentile\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
