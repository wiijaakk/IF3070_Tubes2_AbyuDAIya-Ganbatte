{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Data Preprocessing Pipeline\n",
    "## Tugas Besar 2 IF3070 – Dasar Inteligensi Artifisial\n",
    "\n",
    "**Author:** AbyuDAIya-Ganbatte Team\n",
    "\n",
    "This notebook implements a comprehensive preprocessing pipeline for fraud detection data, including:\n",
    "- Missing value imputation\n",
    "- One-hot encoding\n",
    "- Feature scaling\n",
    "- Outlier handling\n",
    "- Advanced feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '\"c:/Program Files/Python313/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Training data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "print(f\"\\nTraining data columns: {train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of training data:\")\n",
    "display(train.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of test data:\")\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Training data info:\")\n",
    "print(train.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Missing values in training data:\")\n",
    "missing_train = train.isnull().sum()\n",
    "print(missing_train[missing_train > 0])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Missing values in test data:\")\n",
    "missing_test = test.isnull().sum()\n",
    "print(missing_test[missing_test > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution\n",
    "print(\"Target distribution:\")\n",
    "print(train['is_fraud'].value_counts())\n",
    "print(f\"\\nFraud percentage: {train['is_fraud'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "train['is_fraud'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Fraud vs Non-Fraud Cases')\n",
    "plt.xlabel('is_fraud')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Non-Fraud', 'Fraud'], rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store IDs for later use\n",
    "test_ids = test['ID'].values\n",
    "train_ids = train['ID'].values\n",
    "\n",
    "print(f\"Stored {len(train_ids)} training IDs\")\n",
    "print(f\"Stored {len(test_ids)} test IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target variable\n",
    "y_train = train['is_fraud'].values\n",
    "\n",
    "print(f\"Target variable shape: {y_train.shape}\")\n",
    "print(f\"Target variable type: {y_train.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "cols_to_drop = ['ID', 'transaction_id', 'user_id']\n",
    "\n",
    "train_features = train.drop(columns=['is_fraud'] + cols_to_drop, errors='ignore')\n",
    "test_features = test.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Training features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types\n",
    "numerical_cols = train_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train_features.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns ({len(numerical_cols)}):\")\n",
    "print(numerical_cols)\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}):\")\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Missing Value Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in training data\n",
    "train_filled = train_features.copy()\n",
    "imputation_values = {}\n",
    "\n",
    "# Mean imputation for numerical columns\n",
    "for col in numerical_cols:\n",
    "    if col in train_filled.columns and train_filled[col].isnull().any():\n",
    "        mean_value = train_filled[col].mean()\n",
    "        train_filled[col] = train_filled[col].fillna(mean_value)\n",
    "        imputation_values[col] = {'type': 'mean', 'value': mean_value}\n",
    "        print(f\"Imputed {col} with mean: {mean_value:.4f}\")\n",
    "\n",
    "# Mode imputation for categorical columns\n",
    "for col in categorical_cols:\n",
    "    if col in train_filled.columns and train_filled[col].isnull().any():\n",
    "        mode_value = train_filled[col].mode()\n",
    "        if len(mode_value) > 0:\n",
    "            mode_value = mode_value[0]\n",
    "            train_filled[col] = train_filled[col].fillna(mode_value)\n",
    "            imputation_values[col] = {'type': 'mode', 'value': mode_value}\n",
    "            print(f\"Imputed {col} with mode: {mode_value}\")\n",
    "\n",
    "print(f\"\\nImputation complete. Imputed {len(imputation_values)} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same imputation to test data\n",
    "test_filled = test_features.copy()\n",
    "\n",
    "for col, info in imputation_values.items():\n",
    "    if col in test_filled.columns and test_filled[col].isnull().any():\n",
    "        test_filled[col] = test_filled[col].fillna(info['value'])\n",
    "        print(f\"Applied imputation to test set for: {col}\")\n",
    "\n",
    "# Handle any remaining missing values in test (categories not in train)\n",
    "for col in numerical_cols:\n",
    "    if col in test_filled.columns and test_filled[col].isnull().any():\n",
    "        fill_val = imputation_values.get(col, {}).get('value', 0)\n",
    "        test_filled[col] = test_filled[col].fillna(fill_val)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in test_filled.columns and test_filled[col].isnull().any():\n",
    "        mode_val = train_filled[col].mode()[0] if len(train_filled[col].mode()) > 0 else 'unknown'\n",
    "        test_filled[col] = test_filled[col].fillna(mode_val)\n",
    "\n",
    "print(f\"\\nMissing values remaining in train: {train_filled.isnull().sum().sum()}\")\n",
    "print(f\"Missing values remaining in test: {test_filled.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "### 6.1 Ratio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ratio features\n",
    "ratio_pairs = [\n",
    "    ('transaction_amount', 'avg_transaction_amount', 'amount_vs_avg_ratio'),\n",
    "    ('transaction_amount', 'std_transaction_amount', 'amount_vs_std_ratio'),\n",
    "    ('transactions_last_1h', 'transactions_last_24h', 'hourly_vs_daily_ratio'),\n",
    "    ('failed_login_attempts', 'num_prev_transactions', 'failed_vs_total_ratio'),\n",
    "    ('shared_ip_users', 'shared_device_users', 'ip_vs_device_shared_ratio'),\n",
    "]\n",
    "\n",
    "for num, denom, name in ratio_pairs:\n",
    "    if num in train_filled.columns and denom in train_filled.columns:\n",
    "        # Train\n",
    "        denom_safe = train_filled[denom].replace(0, 1e-10)\n",
    "        train_filled[name] = train_filled[num] / denom_safe\n",
    "        \n",
    "        # Test\n",
    "        denom_safe = test_filled[denom].replace(0, 1e-10)\n",
    "        test_filled[name] = test_filled[num] / denom_safe\n",
    "        \n",
    "        print(f\"Created ratio feature: {name}\")\n",
    "\n",
    "print(f\"\\nTotal features after ratio creation: {train_filled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Log-Transformed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log-transformed features for skewed distributions\n",
    "log_candidates = [\n",
    "    'transaction_amount', 'avg_transaction_amount', 'std_transaction_amount',\n",
    "    'account_age_days', 'distance_from_home', 'num_prev_transactions'\n",
    "]\n",
    "\n",
    "for col in log_candidates:\n",
    "    if col in train_filled.columns:\n",
    "        # Only log transform non-negative columns\n",
    "        min_val = train_filled[col].min()\n",
    "        if min_val >= 0:\n",
    "            train_filled[f'{col}_log'] = np.log1p(train_filled[col])\n",
    "            test_filled[f'{col}_log'] = np.log1p(test_filled[col])\n",
    "            print(f\"Created log feature: {col}_log\")\n",
    "\n",
    "print(f\"\\nTotal features after log transformation: {train_filled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Score Interactions\n",
    "if 'ip_risk_score' in train_filled.columns and 'device_trust_score' in train_filled.columns:\n",
    "    train_filled['risk_interaction'] = train_filled['ip_risk_score'] * (1 - train_filled['device_trust_score'] / 100)\n",
    "    test_filled['risk_interaction'] = test_filled['ip_risk_score'] * (1 - test_filled['device_trust_score'] / 100)\n",
    "    print(\"Created: risk_interaction\")\n",
    "\n",
    "if 'merchant_risk' in train_filled.columns and 'country_risk' in train_filled.columns:\n",
    "    train_filled['merchant_country_risk'] = train_filled['merchant_risk'] * train_filled['country_risk']\n",
    "    test_filled['merchant_country_risk'] = test_filled['merchant_risk'] * test_filled['country_risk']\n",
    "    print(\"Created: merchant_country_risk\")\n",
    "\n",
    "# Transaction amount anomaly indicators\n",
    "if 'transaction_amount' in train_filled.columns and 'avg_transaction_amount' in train_filled.columns:\n",
    "    std_col = 'std_transaction_amount'\n",
    "    if std_col in train_filled.columns and train_filled[std_col].mean() > 0:\n",
    "        train_filled['amount_zscore'] = (train_filled['transaction_amount'] - train_filled['avg_transaction_amount']) / (train_filled[std_col] + 1e-6)\n",
    "        test_filled['amount_zscore'] = (test_filled['transaction_amount'] - test_filled['avg_transaction_amount']) / (test_filled[std_col] + 1e-6)\n",
    "        print(\"Created: amount_zscore\")\n",
    "\n",
    "# Velocity-based features\n",
    "if 'transactions_last_24h' in train_filled.columns and 'transactions_last_1h' in train_filled.columns:\n",
    "    train_filled['hourly_concentration'] = train_filled['transactions_last_1h'] / (train_filled['transactions_last_24h'] + 1)\n",
    "    test_filled['hourly_concentration'] = test_filled['transactions_last_1h'] / (test_filled['transactions_last_24h'] + 1)\n",
    "    print(\"Created: hourly_concentration\")\n",
    "\n",
    "# Account trust features\n",
    "if 'account_age_days' in train_filled.columns and 'num_prev_transactions' in train_filled.columns:\n",
    "    train_filled['tx_per_day_age'] = train_filled['num_prev_transactions'] / (train_filled['account_age_days'] + 1)\n",
    "    test_filled['tx_per_day_age'] = test_filled['num_prev_transactions'] / (test_filled['account_age_days'] + 1)\n",
    "    print(\"Created: tx_per_day_age\")\n",
    "\n",
    "# New user risk\n",
    "if 'is_new_country' in train_filled.columns and 'distance_from_home' in train_filled.columns:\n",
    "    train_filled['new_location_distance'] = train_filled['is_new_country'] * train_filled['distance_from_home']\n",
    "    test_filled['new_location_distance'] = test_filled['is_new_country'] * test_filled['distance_from_home']\n",
    "    print(\"Created: new_location_distance\")\n",
    "\n",
    "# Failed login impact\n",
    "if 'failed_login_attempts' in train_filled.columns and 'transaction_amount' in train_filled.columns:\n",
    "    train_filled['failed_login_amount'] = train_filled['failed_login_attempts'] * train_filled['transaction_amount']\n",
    "    test_filled['failed_login_amount'] = test_filled['failed_login_attempts'] * test_filled['transaction_amount']\n",
    "    print(\"Created: failed_login_amount\")\n",
    "\n",
    "# Shared resource risk\n",
    "if 'shared_ip_users' in train_filled.columns and 'shared_device_users' in train_filled.columns:\n",
    "    train_filled['total_shared_users'] = train_filled['shared_ip_users'] + train_filled['shared_device_users']\n",
    "    test_filled['total_shared_users'] = test_filled['shared_ip_users'] + test_filled['shared_device_users']\n",
    "    \n",
    "    train_filled['shared_resource_product'] = train_filled['shared_ip_users'] * train_filled['shared_device_users']\n",
    "    test_filled['shared_resource_product'] = test_filled['shared_ip_users'] * test_filled['shared_device_users']\n",
    "    print(\"Created: total_shared_users, shared_resource_product\")\n",
    "\n",
    "# Time-based risk\n",
    "if 'time_of_day' in train_filled.columns:\n",
    "    train_filled['is_night_time'] = ((train_filled['time_of_day'] >= 0) & (train_filled['time_of_day'] <= 6) | \n",
    "                                      (train_filled['time_of_day'] >= 22)).astype(float)\n",
    "    test_filled['is_night_time'] = ((test_filled['time_of_day'] >= 0) & (test_filled['time_of_day'] <= 6) | \n",
    "                                     (test_filled['time_of_day'] >= 22)).astype(float)\n",
    "    print(\"Created: is_night_time\")\n",
    "\n",
    "if 'day_of_week' in train_filled.columns:\n",
    "    train_filled['is_weekend'] = (train_filled['day_of_week'] >= 5).astype(float)\n",
    "    test_filled['is_weekend'] = (test_filled['day_of_week'] >= 5).astype(float)\n",
    "    print(\"Created: is_weekend\")\n",
    "\n",
    "# Chargeback history interaction\n",
    "if 'has_chargeback_history' in train_filled.columns and 'transaction_amount' in train_filled.columns:\n",
    "    train_filled['chargeback_high_amount'] = train_filled['has_chargeback_history'] * (train_filled['transaction_amount'] > train_filled['transaction_amount'].median()).astype(float)\n",
    "    test_filled['chargeback_high_amount'] = test_filled['has_chargeback_history'] * (test_filled['transaction_amount'] > train_filled['transaction_amount'].median()).astype(float)\n",
    "    print(\"Created: chargeback_high_amount\")\n",
    "\n",
    "# Additional high-value interactions\n",
    "if 'ip_risk_score' in train_filled.columns and 'transaction_amount' in train_filled.columns:\n",
    "    train_filled['high_risk_high_amount'] = train_filled['ip_risk_score'] * train_filled['transaction_amount']\n",
    "    test_filled['high_risk_high_amount'] = test_filled['ip_risk_score'] * test_filled['transaction_amount']\n",
    "    print(\"Created: high_risk_high_amount\")\n",
    "\n",
    "if 'failed_login_attempts' in train_filled.columns and 'ip_risk_score' in train_filled.columns:\n",
    "    train_filled['failed_login_risk'] = train_filled['failed_login_attempts'] * train_filled['ip_risk_score']\n",
    "    test_filled['failed_login_risk'] = test_filled['failed_login_attempts'] * test_filled['ip_risk_score']\n",
    "    print(\"Created: failed_login_risk\")\n",
    "\n",
    "if 'is_new_country' in train_filled.columns and 'transaction_amount' in train_filled.columns:\n",
    "    train_filled['new_country_amount'] = train_filled['is_new_country'] * train_filled['transaction_amount']\n",
    "    test_filled['new_country_amount'] = test_filled['is_new_country'] * test_filled['transaction_amount']\n",
    "    print(\"Created: new_country_amount\")\n",
    "\n",
    "# Squared features for important risk indicators\n",
    "if 'ip_risk_score' in train_filled.columns:\n",
    "    train_filled['ip_risk_squared'] = train_filled['ip_risk_score'] ** 2\n",
    "    test_filled['ip_risk_squared'] = test_filled['ip_risk_score'] ** 2\n",
    "    print(\"Created: ip_risk_squared\")\n",
    "\n",
    "if 'merchant_risk' in train_filled.columns:\n",
    "    train_filled['merchant_risk_squared'] = train_filled['merchant_risk'] ** 2\n",
    "    test_filled['merchant_risk_squared'] = test_filled['merchant_risk'] ** 2\n",
    "    print(\"Created: merchant_risk_squared\")\n",
    "\n",
    "if 'country_risk' in train_filled.columns:\n",
    "    train_filled['country_risk_squared'] = train_filled['country_risk'] ** 2\n",
    "    test_filled['country_risk_squared'] = test_filled['country_risk'] ** 2\n",
    "    print(\"Created: country_risk_squared\")\n",
    "\n",
    "# Combined risk score\n",
    "risk_cols = []\n",
    "if 'ip_risk_score' in train_filled.columns:\n",
    "    risk_cols.append('ip_risk_score')\n",
    "if 'merchant_risk' in train_filled.columns:\n",
    "    risk_cols.append('merchant_risk')\n",
    "if 'country_risk' in train_filled.columns:\n",
    "    risk_cols.append('country_risk')\n",
    "if len(risk_cols) > 0:\n",
    "    train_filled['combined_risk'] = train_filled[risk_cols].mean(axis=1)\n",
    "    test_filled['combined_risk'] = test_filled[risk_cols].mean(axis=1)\n",
    "    print(\"Created: combined_risk\")\n",
    "\n",
    "print(f\"\\nTotal features after interaction creation: {train_filled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update numerical columns list after feature engineering\n",
    "numerical_cols = train_filled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train_filled.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Features after engineering:\")\n",
    "print(f\"  - Numerical: {len(numerical_cols)}\")\n",
    "print(f\"  - Categorical: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on training data\n",
    "train_encoded = train_filled.copy()\n",
    "encoding_info = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in train_encoded.columns:\n",
    "        # Get unique categories\n",
    "        categories = train_encoded[col].unique()\n",
    "        categories = [c for c in categories if pd.notna(c)]\n",
    "        categories = sorted(categories)\n",
    "        \n",
    "        encoding_info[col] = categories\n",
    "        \n",
    "        # Create dummy columns\n",
    "        for category in categories:\n",
    "            new_col_name = f\"{col}_{category}\"\n",
    "            train_encoded[new_col_name] = (train_encoded[col] == category).astype(int)\n",
    "        \n",
    "        # Drop original column\n",
    "        train_encoded = train_encoded.drop(columns=[col])\n",
    "        print(f\"Encoded {col}: {len(categories)} categories\")\n",
    "\n",
    "print(f\"\\nShape after encoding: {train_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same encoding to test data\n",
    "test_encoded = test_filled.copy()\n",
    "\n",
    "for col, categories in encoding_info.items():\n",
    "    if col in test_encoded.columns:\n",
    "        for category in categories:\n",
    "            new_col_name = f\"{col}_{category}\"\n",
    "            test_encoded[new_col_name] = (test_encoded[col] == category).astype(int)\n",
    "        \n",
    "        test_encoded = test_encoded.drop(columns=[col])\n",
    "\n",
    "print(f\"Test shape after encoding: {test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both train and test have same columns\n",
    "train_cols = set(train_encoded.columns)\n",
    "test_cols = set(test_encoded.columns)\n",
    "\n",
    "# Add missing columns to test\n",
    "for col in train_cols - test_cols:\n",
    "    test_encoded[col] = 0\n",
    "    print(f\"Added missing column to test: {col}\")\n",
    "\n",
    "# Add missing columns to train (shouldn't happen often)\n",
    "for col in test_cols - train_cols:\n",
    "    train_encoded[col] = 0\n",
    "    print(f\"Added missing column to train: {col}\")\n",
    "\n",
    "# Ensure same column order\n",
    "all_cols = sorted(train_encoded.columns.tolist())\n",
    "train_encoded = train_encoded[all_cols]\n",
    "test_encoded = test_encoded[all_cols]\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  Train: {train_encoded.shape}\")\n",
    "print(f\"  Test: {test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip outliers using percentile-based winsorization\n",
    "train_values = train_encoded.values.copy()\n",
    "test_values = test_encoded.values.copy()\n",
    "\n",
    "n_features = train_values.shape[1]\n",
    "clip_bounds = {'lower': [], 'upper': []}\n",
    "\n",
    "lower_percentile = 1\n",
    "upper_percentile = 99\n",
    "\n",
    "for i in range(n_features):\n",
    "    lower = np.percentile(train_values[:, i], lower_percentile)\n",
    "    upper = np.percentile(train_values[:, i], upper_percentile)\n",
    "    \n",
    "    clip_bounds['lower'].append(lower)\n",
    "    clip_bounds['upper'].append(upper)\n",
    "    \n",
    "    train_values[:, i] = np.clip(train_values[:, i], lower, upper)\n",
    "\n",
    "print(f\"Clipped outliers in training data using {lower_percentile}th and {upper_percentile}th percentiles\")\n",
    "print(f\"Shape: {train_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same clipping to test data\n",
    "for i in range(test_values.shape[1]):\n",
    "    test_values[:, i] = np.clip(test_values[:, i], clip_bounds['lower'][i], clip_bounds['upper'][i])\n",
    "\n",
    "print(f\"Applied outlier clipping to test data\")\n",
    "print(f\"Shape: {test_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Scaling (Standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and standard deviation from training data\n",
    "means = np.mean(train_values, axis=0)\n",
    "stds = np.std(train_values, axis=0)\n",
    "\n",
    "# Avoid division by zero\n",
    "stds[stds == 0] = 1.0\n",
    "\n",
    "print(f\"Computed scaling parameters:\")\n",
    "print(f\"  Mean range: [{means.min():.4f}, {means.max():.4f}]\")\n",
    "print(f\"  Std range: [{stds.min():.4f}, {stds.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization\n",
    "X_train = (train_values - means) / stds\n",
    "X_test = (test_values - means) / stds\n",
    "\n",
    "print(f\"Standardization complete!\")\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "print(f\"\\nX_train statistics:\")\n",
    "print(f\"  Mean: {X_train.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std: {X_train.std():.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFinal Dataset Shapes:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  test_ids: {test_ids.shape}\")\n",
    "\n",
    "print(f\"\\nNumber of features: {X_train.shape[1]}\")\n",
    "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")\n",
    "\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Non-fraud: {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Fraud: {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Preprocessed Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "np.save('X_train_preprocessed.npy', X_train)\n",
    "np.save('y_train_preprocessed.npy', y_train)\n",
    "np.save('X_test_preprocessed.npy', X_test)\n",
    "np.save('test_ids.npy', test_ids)\n",
    "\n",
    "print(\"Preprocessed data saved successfully!\")\n",
    "print(\"  - X_train_preprocessed.npy\")\n",
    "print(\"  - y_train_preprocessed.npy\")\n",
    "print(\"  - X_test_preprocessed.npy\")\n",
    "print(\"  - test_ids.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessing parameters\n",
    "preprocessing_params = {\n",
    "    'imputation_values': {k: {'type': v['type'], 'value': float(v['value']) if isinstance(v['value'], (int, float, np.number)) else str(v['value'])} \n",
    "                         for k, v in imputation_values.items()},\n",
    "    'encoding_info': {k: [str(cat) for cat in v] for k, v in encoding_info.items()},\n",
    "    'clip_bounds': {'lower': [float(x) for x in clip_bounds['lower']], \n",
    "                    'upper': [float(x) for x in clip_bounds['upper']]},\n",
    "    'scaling_params': {'means': means.tolist(), 'stds': stds.tolist()},\n",
    "    'feature_names': all_cols\n",
    "}\n",
    "\n",
    "with open('preprocessing_params.json', 'w') as f:\n",
    "    json.dump(preprocessing_params, f, indent=2)\n",
    "\n",
    "print(\"Preprocessing parameters saved to preprocessing_params.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Feature Distributions (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of first 5 features after preprocessing\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, X_train.shape[1])):\n",
    "    axes[i].hist(X_train[:, i], bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[i].set_title(f'Feature {i}')\n",
    "    axes[i].set_xlabel('Value (standardized)')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution of First 6 Standardized Features', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Data Ready for Modeling\n",
    "\n",
    "The preprocessed data is now ready to be used with machine learning models. You have:\n",
    "\n",
    "- **X_train**: Preprocessed training features (standardized)\n",
    "- **y_train**: Training labels\n",
    "- **X_test**: Preprocessed test features (standardized)\n",
    "- **test_ids**: Test sample IDs for submission\n",
    "\n",
    "All preprocessing steps performed:\n",
    "1. ✅ Missing value imputation\n",
    "2. ✅ Feature engineering (ratios, logs, interactions)\n",
    "3. ✅ One-hot encoding\n",
    "4. ✅ Outlier clipping\n",
    "5. ✅ Standardization (z-score normalization)\n",
    "\n",
    "You can now proceed to train your machine learning models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
