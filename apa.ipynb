{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection System (Logistic Regression from Scratch)\n",
    "**Tugas Besar 2 IF3070 â€“ Dasar Inteligensi Artifisial**\n",
    "\n",
    "**Team:** AbyuDAIya-Ganbatte\n",
    "\n",
    "This notebook implements a complete Fraud Detection pipeline using **only NumPy and Pandas**. It includes:\n",
    "1.  **Logistic Regression** with Adam Optimizer, Focal Loss, and Elastic Net.\n",
    "2.  **Preprocessing** from scratch (Scaling, Imputation, Encoding).\n",
    "3.  **Feature Engineering** specifically designed for fraud detection.\n",
    "4.  **Evaluation** using manual implementations of ROC-AUC, F1, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer # Only used for complex imputation\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics\n",
    "Implementation of metrics from scratch (Accuracy, Precision, Recall, F1, ROC-AUC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def precision_score(y_true, y_pred):\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    false_positives = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    if true_positives + false_positives == 0: return 0.0\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    false_negatives = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    if true_positives + false_negatives == 0: return 0.0\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    if prec + rec == 0: return 0.0\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "def roc_auc_score(y_true, y_scores):\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_scores = np.array(y_scores).flatten()\n",
    "    \n",
    "    thresholds = np.unique(y_scores)\n",
    "    thresholds = np.concatenate([[thresholds.max() + 1], thresholds, [thresholds.min() - 1]])\n",
    "    thresholds = np.sort(thresholds)[::-1]\n",
    "    \n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    n_pos = np.sum(y_true == 1)\n",
    "    n_neg = np.sum(y_true == 0)\n",
    "    \n",
    "    if n_pos == 0 or n_neg == 0: return 0.5\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_scores >= thresh).astype(int)\n",
    "        tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        tpr_list.append(tp / n_pos)\n",
    "        fpr_list.append(fp / n_neg)\n",
    "    \n",
    "    # Trapezoidal rule\n",
    "    tpr_array = np.array(tpr_list)\n",
    "    fpr_array = np.array(fpr_list)\n",
    "    sorted_indices = np.argsort(fpr_array)\n",
    "    return np.trapz(tpr_array[sorted_indices], fpr_array[sorted_indices])\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    tn = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    fp = np.sum((y_pred == 1) & (y_true == 0))\n",
    "    fn = np.sum((y_pred == 0) & (y_true == 1))\n",
    "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    return np.array([[tn, fp], [fn, tp]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Model\n",
    "Includes Adam Optimizer, Momentum, and Focal Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, optimizer=\"batch\", \n",
    "                 batch_size=None, regularization=0.0, l1_ratio=0.0, class_weight=None,\n",
    "                 lr_schedule=\"constant\", lr_decay=0.1, lr_decay_steps=100,\n",
    "                 momentum=0.0, nesterov=False,\n",
    "                 beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
    "                 use_focal_loss=False, focal_gamma=2.0,\n",
    "                 early_stopping=True, patience=10, tol=1e-5, verbose=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.initial_lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.regularization = regularization\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.class_weight = class_weight\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.lr_decay = lr_decay\n",
    "        self.lr_decay_steps = lr_decay_steps\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.early_stopping = early_stopping\n",
    "        self.patience = patience\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        if batch_size is None:\n",
    "            self.batch_size = 32 if self.optimizer == \"mini-batch\" else None\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "        \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.velocity_w = None\n",
    "        self.velocity_b = None\n",
    "        self.m_w = None\n",
    "        self.v_w = None\n",
    "        self.m_b = None\n",
    "        self.v_b = None\n",
    "        self.t = 0\n",
    "        self.class_weights_ = None\n",
    "        self.loss_history = []\n",
    "        self.weight_history = []\n",
    "        self.lr_history = []\n",
    "    \n",
    "    def _get_learning_rate(self, iteration):\n",
    "        if self.lr_schedule == \"constant\": return self.initial_lr\n",
    "        elif self.lr_schedule == \"step\":\n",
    "            return self.initial_lr * (self.lr_decay ** (iteration // self.lr_decay_steps))\n",
    "        elif self.lr_schedule == \"exponential\":\n",
    "            return self.initial_lr * (self.lr_decay ** (iteration / self.n_iterations))\n",
    "        elif self.lr_schedule == \"cosine\":\n",
    "            return self.initial_lr * (1 + np.cos(np.pi * iteration / self.n_iterations)) / 2\n",
    "        return self.initial_lr\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        positive_mask = z >= 0\n",
    "        negative_mask = ~positive_mask\n",
    "        result = np.zeros_like(z, dtype=float)\n",
    "        result[positive_mask] = 1 / (1 + np.exp(-z[positive_mask]))\n",
    "        exp_z = np.exp(z[negative_mask])\n",
    "        result[negative_mask] = exp_z / (1 + exp_z)\n",
    "        return result\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred, sample_weights=None):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        if self.use_focal_loss:\n",
    "            p_t = np.where(y_true == 1, y_pred, 1 - y_pred)\n",
    "            focal_weight = (1 - p_t) ** self.focal_gamma\n",
    "            loss_per_sample = -focal_weight * np.log(p_t)\n",
    "        else:\n",
    "            loss_per_sample = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        \n",
    "        if sample_weights is not None:\n",
    "            loss = np.sum(sample_weights * loss_per_sample) / np.sum(sample_weights)\n",
    "        else:\n",
    "            loss = np.mean(loss_per_sample)\n",
    "            \n",
    "        if self.regularization > 0 and self.weights is not None:\n",
    "            l1_term = self.l1_ratio * np.sum(np.abs(self.weights))\n",
    "            l2_term = (1 - self.l1_ratio) * 0.5 * np.sum(self.weights ** 2)\n",
    "            loss += self.regularization * (l1_term + l2_term)\n",
    "        return loss\n",
    "    \n",
    "    def _compute_gradients(self, X, y, y_pred, sample_weights=None):\n",
    "        n_samples = len(y)\n",
    "        if self.use_focal_loss:\n",
    "            epsilon = 1e-15\n",
    "            y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "            p_t = np.where(y == 1, y_pred_clipped, 1 - y_pred_clipped)\n",
    "            focal_weight = (1 - p_t) ** self.focal_gamma\n",
    "            grad_p = np.where(\n",
    "                y == 1,\n",
    "                -focal_weight * (self.focal_gamma * (1 - y_pred_clipped) * np.log(y_pred_clipped + epsilon) + 1) / (y_pred_clipped + epsilon),\n",
    "                focal_weight * (self.focal_gamma * y_pred_clipped * np.log(1 - y_pred_clipped + epsilon) + 1) / (1 - y_pred_clipped + epsilon)\n",
    "            )\n",
    "            error = grad_p * y_pred_clipped * (1 - y_pred_clipped)\n",
    "        else:\n",
    "            error = y_pred - y\n",
    "        \n",
    "        if sample_weights is not None:\n",
    "            weighted_error = sample_weights * error\n",
    "            dw = np.dot(X.T, weighted_error) / np.sum(sample_weights)\n",
    "            db = np.sum(weighted_error) / np.sum(sample_weights)\n",
    "        else:\n",
    "            dw = (1 / n_samples) * np.dot(X.T, error)\n",
    "            db = (1 / n_samples) * np.sum(error)\n",
    "            \n",
    "        if self.regularization > 0:\n",
    "            l2_grad = (1 - self.l1_ratio) * self.weights\n",
    "            l1_grad = self.l1_ratio * np.sign(self.weights)\n",
    "            dw += self.regularization * (l1_grad + l2_grad)\n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if isinstance(X, pd.DataFrame): X = X.values\n",
    "        if isinstance(y, (pd.Series, pd.DataFrame)): y = y.values.flatten()\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        np.random.seed(42)\n",
    "        self.weights = np.random.randn(n_features) * np.sqrt(2.0 / n_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        # Adam Init\n",
    "        self.m_w = np.zeros(n_features)\n",
    "        self.v_w = np.zeros(n_features)\n",
    "        self.m_b = 0.0\n",
    "        self.v_b = 0.0\n",
    "        self.t = 0\n",
    "        \n",
    "        if self.class_weight == \"balanced\":\n",
    "            class_counts = np.bincount(y.astype(int))\n",
    "            self.class_weights_ = n_samples / (2 * class_counts)\n",
    "            sample_weights = np.where(y == 1, self.class_weights_[1], self.class_weights_[0])\n",
    "        else:\n",
    "            sample_weights = None\n",
    "        \n",
    "        if self.optimizer == \"adam\":\n",
    "            self._fit_adam(X, y, sample_weights)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only Adam implemented in this notebook view for brevity\")\n",
    "        return self\n",
    "\n",
    "    def _fit_adam(self, X, y, sample_weights=None):\n",
    "        n_samples = len(y)\n",
    "        batch_size = min(self.batch_size if self.batch_size else 32, n_samples)\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        best_weights = self.weights.copy()\n",
    "        \n",
    "        for iteration in range(self.n_iterations):\n",
    "            lr = self._get_learning_rate(iteration)\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled, y_shuffled = X[indices], y[indices]\n",
    "            sw_shuffled = sample_weights[indices] if sample_weights is not None else None\n",
    "            \n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                sw_batch = sw_shuffled[start_idx:end_idx] if sw_shuffled is not None else None\n",
    "                \n",
    "                z = np.dot(X_batch, self.weights) + self.bias\n",
    "                y_pred = self.sigmoid(z)\n",
    "                dw, db = self._compute_gradients(X_batch, y_batch, y_pred, sw_batch)\n",
    "                \n",
    "                self.t += 1\n",
    "                self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * dw\n",
    "                self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * db\n",
    "                self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (dw ** 2)\n",
    "                self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (db ** 2)\n",
    "                \n",
    "                m_w_corr = self.m_w / (1 - self.beta1 ** self.t)\n",
    "                m_b_corr = self.m_b / (1 - self.beta1 ** self.t)\n",
    "                v_w_corr = self.v_w / (1 - self.beta2 ** self.t)\n",
    "                v_b_corr = self.v_b / (1 - self.beta2 ** self.t)\n",
    "                \n",
    "                self.weights -= lr * m_w_corr / (np.sqrt(v_w_corr) + self.epsilon)\n",
    "                self.bias -= lr * m_b_corr / (np.sqrt(v_b_corr) + self.epsilon)\n",
    "            \n",
    "            z_full = np.dot(X, self.weights) + self.bias\n",
    "            loss = self.compute_loss(y, self.sigmoid(z_full), sample_weights)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            if self.early_stopping:\n",
    "                if loss < best_loss - self.tol:\n",
    "                    best_loss = loss\n",
    "                    best_weights = self.weights.copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        if self.verbose: print(f\"Early stopping at {iteration}, loss: {loss:.6f}\")\n",
    "                        self.weights = best_weights\n",
    "                        break\n",
    "                        \n",
    "            if self.verbose and iteration % 500 == 0:\n",
    "                print(f\"Iteration {iteration}: loss = {loss:.6f}\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame): X = X.values\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        model_data = {\n",
    "            'weights': self.weights.tolist(), 'bias': float(self.bias), \n",
    "            'learning_rate': self.learning_rate, 'n_iterations': self.n_iterations\n",
    "        }\n",
    "        with open(filename, 'w') as f: json.dump(model_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Utils & Feature Engineering\n",
    "Splitters, Imputers, Scalers, and Feature Generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None, stratify=None):\n",
    "    if isinstance(X, pd.DataFrame): X = X.values\n",
    "    if isinstance(y, (pd.Series, pd.DataFrame)): y = y.values.flatten()\n",
    "    n_samples = len(X)\n",
    "    if random_state: np.random.seed(random_state)\n",
    "    \n",
    "    if stratify is not None:\n",
    "        if isinstance(stratify, (pd.Series, pd.DataFrame)): stratify = stratify.values.flatten()\n",
    "        train_idx, test_idx = [], []\n",
    "        for cls in np.unique(stratify):\n",
    "            cls_idx = np.where(stratify == cls)[0]\n",
    "            np.random.shuffle(cls_idx)\n",
    "            n_test_cls = int(len(cls_idx) * test_size)\n",
    "            test_idx.extend(cls_idx[:n_test_cls])\n",
    "            train_idx.extend(cls_idx[n_test_cls:])\n",
    "        train_idx, test_idx = np.array(train_idx), np.array(test_idx)\n",
    "        np.random.shuffle(train_idx)\n",
    "        np.random.shuffle(test_idx)\n",
    "    else:\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        n_test = int(n_samples * test_size)\n",
    "        test_idx, train_idx = indices[:n_test], indices[n_test:]\n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        self.means = None\n",
    "        self.stds = None\n",
    "    def fit_transform(self, X):\n",
    "        self.means = np.mean(X, axis=0)\n",
    "        self.stds = np.std(X, axis=0)\n",
    "        self.stds[self.stds == 0] = 1.0\n",
    "        return (X - self.means) / self.stds\n",
    "    def transform(self, X):\n",
    "        return (X - self.means) / self.stds\n",
    "\n",
    "def clip_outliers(X, lower=1, upper=99):\n",
    "    X_c = X.copy()\n",
    "    bounds = {'lower': [], 'upper': []}\n",
    "    for i in range(X.shape[1]):\n",
    "        l, u = np.percentile(X[:, i], lower), np.percentile(X[:, i], upper)\n",
    "        bounds['lower'].append(l); bounds['upper'].append(u)\n",
    "        X_c[:, i] = np.clip(X[:, i], l, u)\n",
    "    return X_c, bounds\n",
    "\n",
    "def apply_clip(X, bounds):\n",
    "    X_c = X.copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        X_c[:, i] = np.clip(X[:, i], bounds['lower'][i], bounds['upper'][i])\n",
    "    return X_c\n",
    "\n",
    "def handle_missing(df, numerical, categorical, n_neighbors=5):\n",
    "    df_filled = df.copy()\n",
    "    vals = {}\n",
    "    # KNN Imputation\n",
    "    num_cols = [c for c in numerical if c in df_filled.columns]\n",
    "    if num_cols and df_filled[num_cols].isnull().any().any():\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        df_filled[num_cols] = imputer.fit_transform(df_filled[num_cols])\n",
    "        vals['knn'] = imputer\n",
    "        vals['num_cols'] = num_cols\n",
    "    \n",
    "    # Mode Imputation\n",
    "    for col in categorical:\n",
    "        if col in df_filled.columns:\n",
    "            mode = df_filled[col].mode()[0]\n",
    "            df_filled[col] = df_filled[col].fillna(mode)\n",
    "            vals[f'mode_{col}'] = mode\n",
    "    return df_filled, vals\n",
    "\n",
    "def apply_imputation(df, vals):\n",
    "    df_f = df.copy()\n",
    "    if 'knn' in vals:\n",
    "        cols = [c for c in vals['num_cols'] if c in df_f.columns]\n",
    "        df_f[cols] = vals['knn'].transform(df_f[cols])\n",
    "    for k, v in vals.items():\n",
    "        if k.startswith('mode_') and k[5:] in df_f.columns:\n",
    "            df_f[k[5:]] = df_f[k[5:]].fillna(v)\n",
    "    return df_f\n",
    "\n",
    "def one_hot_encode(df, cols):\n",
    "    df_enc = df.copy()\n",
    "    info = {}\n",
    "    for col in cols:\n",
    "        if col in df_enc.columns:\n",
    "            cats = sorted([c for c in df_enc[col].unique() if pd.notna(c)])\n",
    "            info[col] = cats\n",
    "            for c in cats:\n",
    "                df_enc[f\"{col}_{c}\"] = (df_enc[col] == c).astype(int)\n",
    "            df_enc.drop(columns=[col], inplace=True)\n",
    "    return df_enc, info\n",
    "\n",
    "def apply_one_hot(df, info):\n",
    "    df_enc = df.copy()\n",
    "    for col, cats in info.items():\n",
    "        if col in df_enc.columns:\n",
    "            for c in cats:\n",
    "                df_enc[f\"{col}_{c}\"] = (df_enc[col] == c).astype(int)\n",
    "            df_enc.drop(columns=[col], inplace=True)\n",
    "    return df_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Generating ratios, logs, and interaction features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    # Ratios\n",
    "    if 'transaction_amount' in df.columns and 'avg_transaction_amount' in df.columns:\n",
    "        df['amount_vs_avg'] = df['transaction_amount'] / (df['avg_transaction_amount'] + 1e-5)\n",
    "    if 'transactions_last_1h' in df.columns and 'transactions_last_24h' in df.columns:\n",
    "        df['hourly_conc'] = df['transactions_last_1h'] / (df['transactions_last_24h'] + 1)\n",
    "        \n",
    "    # Logs\n",
    "    for col in ['transaction_amount', 'distance_from_home']:\n",
    "        if col in df.columns and df[col].min() >= 0:\n",
    "            df[f'{col}_log'] = np.log1p(df[col])\n",
    "            \n",
    "    # Interactions\n",
    "    if 'ip_risk_score' in df.columns and 'device_trust_score' in df.columns:\n",
    "        df['risk_interaction'] = df['ip_risk_score'] * (1 - df['device_trust_score'] / 100)\n",
    "    if 'failed_login_attempts' in df.columns and 'transaction_amount' in df.columns:\n",
    "        df['failed_login_impact'] = df['failed_login_attempts'] * df['transaction_amount']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Pipeline\n",
    "Data loading, preprocessing execution, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(train_df, test_df):\n",
    "    # Drop IDs\n",
    "    ids = test_df['ID'].values\n",
    "    drop_cols = ['ID', 'transaction_id', 'user_id']\n",
    "    y_train = train_df['is_fraud'].values\n",
    "    \n",
    "    train_X = train_df.drop(columns=['is_fraud'] + drop_cols, errors='ignore')\n",
    "    test_X = test_df.drop(columns=drop_cols, errors='ignore')\n",
    "    \n",
    "    num_cols = train_X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = train_X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # 1. Impute\n",
    "    print(\"Imputing...\")\n",
    "    train_filled, vals = handle_missing(train_X, num_cols, cat_cols, n_neighbors=5)\n",
    "    test_filled = apply_imputation(test_X, vals)\n",
    "    \n",
    "    # 2. Engineer\n",
    "    print(\"Engineering Features...\")\n",
    "    train_eng = engineer_features(train_filled)\n",
    "    test_eng = engineer_features(test_filled)\n",
    "    \n",
    "    # 3. Encode\n",
    "    print(\"Encoding...\")\n",
    "    train_enc, enc_info = one_hot_encode(train_eng, cat_cols)\n",
    "    test_enc = apply_one_hot(test_eng, enc_info)\n",
    "    \n",
    "    # Align columns\n",
    "    cols = sorted(list(set(train_enc.columns) | set(test_enc.columns)))\n",
    "    for c in cols:\n",
    "        if c not in train_enc.columns: train_enc[c] = 0\n",
    "        if c not in test_enc.columns: test_enc[c] = 0\n",
    "    train_enc = train_enc[cols]\n",
    "    test_enc = test_enc[cols]\n",
    "    \n",
    "    # 4. Clip & Scale\n",
    "    print(\"Scaling...\")\n",
    "    X_train_np, bounds = clip_outliers(train_enc.values)\n",
    "    X_test_np = apply_clip(test_enc.values, bounds)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_np)\n",
    "    X_test_scaled = scaler.transform(X_test_np)\n",
    "    \n",
    "    return X_train_scaled, y_train, X_test_scaled, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded.\n",
      "Imputing...\n",
      "Engineering Features...\n",
      "Encoding...\n",
      "Scaling...\n",
      "Train Shape: (100000, 60), Test Shape: (100000, 60)\n",
      "Training Model...\n",
      "Iteration 0: loss = 0.680445\n",
      "Early stopping at 34, loss: 0.674819\n",
      "Validation ROC AUC: 0.6048\n",
      "Submission saved to submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Persada\\AppData\\Local\\Temp\\ipykernel_21040\\2176628005.py:54: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return np.trapz(tpr_array[sorted_indices], fpr_array[sorted_indices])\n"
     ]
    }
   ],
   "source": [
    "# === EXECUTION ===\n",
    "\n",
    "# 1. Load Data\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    print(\"Data Loaded.\")\n",
    "    \n",
    "    # 2. Run Preprocessing\n",
    "    X_train, y_train, X_test, test_ids = preprocess_pipeline(train_df, test_df)\n",
    "    print(f\"Train Shape: {X_train.shape}, Test Shape: {X_test.shape}\")\n",
    "    \n",
    "    # 3. Split Validation\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify=y_train)\n",
    "    \n",
    "    # 4. Train Model\n",
    "    print(\"Training Model...\")\n",
    "    model = LogisticRegression(\n",
    "        learning_rate=0.0015, n_iterations=3000, optimizer=\"adam\", \n",
    "        batch_size=256, regularization=0.0006, l1_ratio=0.5,\n",
    "        class_weight=\"balanced\"\n",
    "    )\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # 5. Evaluate\n",
    "    y_prob_val = model.predict_proba(X_val)\n",
    "    val_auc = roc_auc_score(y_val, y_prob_val)\n",
    "    print(f\"Validation ROC AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    # 6. Submission\n",
    "    final_probs = model.predict_proba(X_test)\n",
    "    submission = pd.DataFrame({'ID': test_ids, 'is_fraud': final_probs})\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved to submission.csv\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv or test.csv not found in current directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
